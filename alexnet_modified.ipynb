{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import LoadDataset as LD\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from scipy.misc import imresize\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 6\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory added\n",
      "['c0', 'c1', 'c2']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LoadData' object has no attribute 'generate_data_2cls'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-57f0200c5a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mload_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/mdabdulkadir/WS1819/ADAI/imagetest'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# give the directory address.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_dat\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mY_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_data_2cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_dat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoadData' object has no attribute 'generate_data_2cls'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "n_categories = 2\n",
    "categories = np.array(['c0', 'c1'])\n",
    "\n",
    "load_data = LD.LoadData('/Users/mdabdulkadir/WS1819/ADAI/imagetest') # give the directory address.\n",
    "X_dat ,Y_dat = load_data.generate_data_2cls()\n",
    "\n",
    "X_dat, Y_dat = load_data.shuffle_data(X_dat,Y_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22424, 227, 227, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X_dat)\n",
    "Y = np.array(Y_dat)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 0, 6, 4, 8, 3, 7, 1, 9, 8, 1, 8, 1, 5, 5, 8, 2, 8, 2, 8, 7,\n",
       "       2, 1, 2, 4, 6, 5, 4, 4, 3, 5, 7, 9, 3, 0, 1, 5, 5, 4, 8, 6, 7, 7,\n",
       "       9, 5, 5, 0, 2, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(img, alpha_std=0.1):\n",
    "    orig_img = img.astype(float).copy()\n",
    "    img = img / 255.0  # rescale to 0 to 1 range\n",
    "    # flatten image to columns of RGB\n",
    "    img_rs = img.reshape(-1, 3)\n",
    "    # img_rs shape (640000, 3)\n",
    "    img_centered = img_rs - np.mean(img_rs, axis=0)\n",
    "    # paper says 3x3 covariance matrix\n",
    "    img_cov = np.cov(img_centered, rowvar=False)\n",
    "    # eigen values and eigen vectors\n",
    "    eig_vals, eig_vecs = np.linalg.eigh(img_cov)\n",
    "    # sort values and vector\n",
    "    sort_perm = eig_vals[::-1].argsort()\n",
    "    eig_vals[::-1].sort()\n",
    "    eig_vecs = eig_vecs[:, sort_perm]\n",
    "\n",
    "    # get [p1, p2, p3]\n",
    "    m1 = np.column_stack((eig_vecs))\n",
    "\n",
    "    # get 3x1 matrix of eigen values multiplied by random variable draw from normal\n",
    "    # distribution with mean of 0 and standard deviation of 0.1\n",
    "    m2 = np.zeros((3, 1))\n",
    "    # according to the paper alpha should only be draw once per augmentation (not once per channel)\n",
    "    alpha = np.random.normal(0, alpha_std)\n",
    "\n",
    "    # broad cast to speed things up\n",
    "    m2[:, 0] = alpha * eig_vals[:]\n",
    "\n",
    "    # this is the vector that we're going to add to each pixel in a moment\n",
    "    add_vect = np.matrix(m1) * np.matrix(m2)\n",
    "\n",
    "    for idx in range(3):   # RGB\n",
    "        orig_img[..., idx] += add_vect[idx]\n",
    "    orig_img = np.clip(orig_img, 0.0, 255.0)\n",
    "    orig_img = orig_img.astype(np.uint8)\n",
    "    return orig_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 227 3\n",
      "(20000, 227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train) = (X[0:20000], Y[0:20000])\n",
    "(x_test, y_test) = (X[20000:], Y[20000:])\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "print(img_height, img_width, channel)\n",
    "\n",
    "x_train = pca(x_train)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "# # for cifar10 dataset\n",
    "# # Load CIFAR10 Data\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# print(x_train.shape)\n",
    "\n",
    "# img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# # convert to one hot encoing\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet Define the Model\n",
    "model = Sequential()\n",
    "# model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='valid', input_shape=(img_height, img_width, channel,)))\n",
    "# for original Alexnet\n",
    "model.add(Conv2D(96, (3,3), strides=(2,2), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "# Local Response normalization for Original Alexnet\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (5,5), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "# Local Response normalization for Original Alexnet\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "# Local Response normalization for Original Alexnet\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 114, 114, 96)      2688      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 57, 57, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 57, 57, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 57, 57, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              177213440 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 197,753,098\n",
      "Trainable params: 197,751,882\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 2424 samples\n",
      "Epoch 1/6\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 14.3562 - acc: 0.1032 - val_loss: 14.4823 - val_acc: 0.1015\n",
      "Epoch 2/6\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 14.4491 - acc: 0.1036 - val_loss: 14.4823 - val_acc: 0.1015\n",
      "Epoch 3/6\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 14.4491 - acc: 0.1036 - val_loss: 14.4823 - val_acc: 0.1015\n",
      "Epoch 4/6\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 14.4491 - acc: 0.1036 - val_loss: 14.4823 - val_acc: 0.1015\n",
      "Epoch 5/6\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 14.4491 - acc: 0.1036 - val_loss: 14.4823 - val_acc: 0.1015\n",
      "Epoch 6/6\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 14.4491 - acc: 0.1036 - val_loss: 14.4823 - val_acc: 0.1015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f39fc94af98>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the model summary\n",
    "model.summary()\n",
    "\n",
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 14.482347881833318\n",
      "Test accuracy: 0.10148514852714617\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
